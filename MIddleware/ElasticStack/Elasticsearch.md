### Elasticsearch

#### 集群

集群由一个或多个拥有相同 cluster.name 配置的节点组成，它们共同承担数据和负载压力。当有节点加入集群或从集群中移除节点时，集群将会重新平均分布所有的数据

##### API

###### 集群

| method |         url          |            params             |    作用    |
| :----: | :------------------: | :---------------------------: | :--------: |
|  GET   |  `/_cluster/health`  |                               |  集群健康  |
|  PUT   | `/{index}/_settings` | {"number_of_replicas": {num}} | 修改副本数 |
|        |                      |                               |            |

##### 节点

###### 主节点

当一个节点（运行中的 Elasticsearch 实例）被选举成主节点时，它将负责管理集群范围内的所有变更（索引的增加、删除，节点的增加、删除），而不涉及到文档级别的变更和搜索等操作

客户端可以将请求发送到集群中的任何节点，包括主节点。每个节点都知道任意文档所处的位置，将请求直接转发到存储文档的节点。并将结果返回（直接在服务端完成转发）

##### 分布式特性

屏蔽了分布式系统的复杂性

-   分配文档到不同的容器或分片中，文档可以储存在一个或多个节点中
-   按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡
-   复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失
-   将集群中任一节点的请求路由到存有相关数据的节点
-   集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复

###### 搜索执行流程

对于单文档的 CRUD 操作，文档的唯一性由 `_index`、`_type`、`routing values`（默认 `_id`）的组合确定。搜索执行模型 query then fetch：

* 查询阶段

  查询会广播到索引中每一个分片拷贝（主分片或副本分片，这些文档可能在集群的任何分片上，一个搜索请求必须询问索引的所有分片的某个副本来确定它们是否含有任何匹配的文档），每个分片在本地执行搜索并构建一个匹配文档的优先队列（一个优先队列仅仅是一个存有 top-n 匹配文档的有序列表，大小取决于分页参数）：

  1. 客户端发送查询请求到节点（当搜索请求被发送到某个节点时，这个节点就变成了协调节点。这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果并返回给客户端），节点创建一个 from + size 的空优先队列
  2. 节点将查询请求转发到索引的每个主分片或副本分片（查询请求可以被某个主分片或某个副本分片处理，协调节点轮询所有的分片拷贝来分摊负载）。每个分片在本地执行查询并添加结果到大小为 from + size 的本地优先队列中
  3. 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表

* 取回阶段

  1. 协调节点判断（丢弃分页不需要的数据）那些文档需要被取回并向相关的分片（给持有相关文档的每个分片创建一个 multi-get request（mget））提交多个 GET 请求
  2. 每个分片加载文档（_source 字段，如果有需要，填充元数据和高亮结果），返回文档给协调节点
  3. 协调节点等待所有文档被取回，协调节点返回结果给客户端

分布式搜索时可用参数进行控制搜索过程：

* preference

  用来控制由哪些分片或节点来处理搜索请求，接受：`_primary`、`_primary_first`、`_local`、`_only_node:xyz`、`_prefer_node:xyz`、`_shards:2,3`，可以使用某些随机字符串来避免 bouncing results（文档具有相同字段值，搜索结果使用该字段来进行排序，可能在主分片处理请求时，文档是一种顺序，副本分片是另外一种顺序，造成用户刷新页面时，搜索结果表现是不同的顺序）让同一个用户始终使用同一个分片，这样可以避免该情况。设置 preference 为会话 id 可以解决这种情况

* timeout

  分片处理完它匹配的所有数据后才会将结果返回给协调节点，即总时间为最慢的分片用时加结果合并的时间。该参数指定分片允许处理数据的最大时间，如果分片超时，则结果可能是部分的，甚至是空数据。搜索的返回结果会用属性 `timed_out` 来表明分片是否超时

* routing

  搜索时，不搜索索引的所有分片，通过指定几个 routing 值来限定只搜索几个相关的分片

  ```
  GET /_search?routing=user_1,user_2
  ```

* search_type

  指定搜索模式，未指定时默认：query_then_fetch，支持 dfs_query_then_fetch（有预查询阶段，这个阶段可以从所有相关分片获取词频率来计算全局词频）

###### 分片

一个分片是一个底层的工作单元，它仅保存了全部数据中的一部分。一个分片是一个 Lucene 的实例，它本身就是一个完整的搜索引擎。 文档被存储和索引到分片内，应用程序是直接与索引而不是与分片进行交互

ES 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 集群规模扩大或者缩小时， ES 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里

一个分片可以是：

*    主分片

     索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量（技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考：使用场景、使用硬件， 文档大小和复杂程度，索引和查询文档的方式以及你期望的响应时长）

     路由一个文档到一个分片时：shard = hash(routing) % number_of_primary_shards。所有的文档 API（get、index、delete、bulk、update、mget）都可以接受 routing 路由参数，通过这个参数可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档都被存储到同一个分片中。

*   副本分片

    是主分片的拷贝，硬件故障的冗余备份，并提供搜索和返回文档等读操作服务，节点丢失后，其他节点的副分片会提升为主分片

在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改（在相同节点数目的集群上增加更多的副本分片不能提高性能）。

新建、索引、删除请求都是写操作，必须在主分片上完成之后才能被复制到相关的副本分片。客户端收到成功响应时，文档变更已经在主副分片执行完成，变更是安全的。

在默认设置下，即使仅仅是在试图执行一个写操作之前，主分片都会要求必须要有规定数量（int (primary + number_of_replicas) / 2) + 1，number_of_replicas 是在索引设置中的设定副本分片数，而不是当前处理活动状态的副本分片数，只有当 number_of_replicas 大于 1 时，规定数量才会执行）的分片副本处于活跃可用状态，才会去执行写操作。

#### 数据

文档的每个字段的所有数据都是默认被索引的。即每个字段都有为了快速检索设置的专用倒排索引。它能在同一个查询中使用所有这些倒排索引。字段的名字可以是任何合法的字符串，不可以包含英文句号 .

ES 中数据可以分为：

*   精确值

    确定的值（日期，ID），字符串也可以表示精确值，精确值很容易查询，结果是二进制的：要么匹配，要么不匹配。

*   全文

    在于该文档匹配查询的程度有多大，即相关性。

##### 分析

###### 倒排索引

ES 使用倒排索引的结构，一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有 一个包含它的文档列表

为了创建倒排索引，首先会将每个文档拆分成单独的词（词条或 tokens），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在那个文档。只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式

###### 分析

分析将一个全文域分成适合倒排索引的独立词条，之后将这些词条统一化为标准格式以提高搜索性（recall）。

1. 分析器首先将字符串按顺序通过每个字符过滤器（一个字符过滤器可以用来去掉 HTML，或将 & 转化成 and），在分词前整理字符串。
2. 其次字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点时，可能会将文本拆分成词条
3. 最后词条按顺序通过每个 token 过滤器，这个过程可能会改变词条，删除词条，增加词条。

ES 内置了开箱即用的字符过滤器、分词器、token 过滤器

###### 内置分析器

索引文档时，它的全文域被分析成词条以用来创建倒排索引。全文域搜索时，需要将查询字符串通过相同的分析过程，以保证搜索的词条格式与索引中的词条格式一致。查询精确域时不会分析查询字符串，而是搜索指定的精确值

*   标准分析器（standard）

    默认使用分析器。根据 Unicode 定义的单词边界划分文本。删除大部分标点，将词条小写。当 ES 在文档中检测到一个新的字符串域时，它会自动设置其为一个全文字符串域，使用标准分析器进行分析

*   简单分析器（simple）

    在任何不是字母的地方分隔文本，将词条小写

*   空格分析器（whitespace）

    在空格的地方划分文本

*   语言分析

    特定语言分析器可以考虑指定语言的特点（英语分析器附带了一组英语无用词：and the，它们会被删除，可以提取单词的词干）

###### Analyze API 

查看文本分析

```json
GET /_analyze
{
  "analyzer": "standard",
  "text": "Text to analyze"
}
{
	"tokens": [
      {
         "token":        "text",
         "start_offset": 0,
         "end_offset":   4,
         "type":         "<ALPHANUM>",
         "position":     1
      },
      {
         "token":        "to",
         "start_offset": 5,
         "end_offset":   7,
         "type":         "<ALPHANUM>",
         "position":     2
      },
      {
         "token":        "analyze",
         "start_offset": 8,
         "end_offset":   15,
         "type":         "<ALPHANUM>",
         "position":     3
      }
   ]
}
```

*   token 是实际存储到索引中的词条
*   position 指词条在原始文本中出现的位置
*   start_offset/end_offset 指明字符在原始字符串中位置
*   type 为每个分析器独有

###### 映射

当 ES 在文档中检测到一个新的字符串域，会自动设置其为一个全文字符串域，使用标准分析器对它进行分析。如果要一个字符串域索引精确值或不太的分析器，必须手动指定映射

| method |        url        |         描述         |
| :----: | :---------------: | :------------------: |
|  GET   | /{index}/_mapping | 获取索引文档字段映射 |

索引中每个文档都有类型，每种类型都有自己的映射或模式定义。映射定义了类型中的域，每个域的数据类型及 ES 如何处理域，映射也用于配置与类型有关的元数据。ES 支持如下简单域类型（当索引一个之前未曾出现的域时，ES 会使用动态映射通过 JSON 中基本数据类型进行映射，如果通过 `"123"` 索引一个数字，它会被映射为 string 类型，而不是 long，如果这个域已经映射为 long。ES 会将这个字符串转化为 long，如果无法转化，则抛出一个异常）：

映射的最高一层被称为根对象，可能包含：

*   properties 节点，列出了文档中可能包含的每个字段的映射

    主要包含：type（字段的数据类型）、index（字段是否应被当成全文（analyzed）来搜索，或准确值（not_analyzed）或不可搜索（no））、analyzer（确定在索引和搜索时全文字段使用的 analyzer）

*   各种元数据字段（以下划线开头）

    _source 字段存储代表文档体的 JSON 字符串

    ```
    PUT /{index}
    {
    	"mappings": {
    		"_doc": {
    			"_source": {
    				// 禁用 source 字段
    				"enabled": false
    			}
    		}
    	}
    }
    ```

    _all 字段，把其他字段值当作一个大字符串来索引的特殊字段。query_string 在没有指定字段时默认使用 _all 字段。include_in_all 设置来控制字段是否要包含在 _all 字段中。默认 true。_all 字段仅仅是一个经过分词的 string 字段。使用默认分词器来分析它的值。不管这个值原本所在字段指定的分词器。

    ```
    PUT /{index}/_doc/_mapping
    {
    	"_doc": {
    		"include_in_all": false,
    		"properties": {
    			"field": {
    				"type": "string",
    				"include_in_all": true    // 	手动包含某个字段到 _all 字段
    			}
    		}
    	},
    	"_all": {"analyzer": "whitespace"} // 设置 all 字段分析器
    }
    ```

    文档标识相关元数据字段（`_id`、`_type`、`_index`、`_uid`），默认情况下，`_uid` 字段可搜索。`_id`、`_index` 字段既没有被索引也没有被存储。

*   设置项，控制如何处理新的字段

*   其他设置，可以同时应用在根对象和其他 object 类型的字段上



*ES简单域类型与JSON类型映射*

| ES 数据类型 |             ES             |           JSON 类型            |
| :---------: | :------------------------: | :----------------------------: |
|   字符串    |  string/6.0 及之后为 text  |             string             |
|    整数     | byte、short、integer、long |              long              |
|   浮点数    |       float、double        |             double             |
|    布尔     |          boolean           |           true/false           |
|    日期     |            date            | 字符串，有效日期（2014-09-15） |

自定义映射，对于不是 string 的域，只需要设置 type，其 index 值为 no 和 not_analyzed，它们永远不会被分析

```json
{
	"number_of_clicks": {
		"type": "integer"
	}
}
```

默认，string 类型域会被认为包含全文。它们的值在索引前，会通过一个分析器，针对这个域的查询在搜索前也会通过一个分析器，string 域映射的两个最重要的属性是 index（控制怎样索引字符串：analyzed（默认）：分析字符串，然后索引，即全文索引；not_analyzed：索引这个域，使其能被搜索，但索引的是精确值，不会进行分析，no：不索引这个域，这个域不会被搜索到）、analyzer，指定分析器，默认 standard 

```json
{
	"tag": {
		"type": "string",
		"index": "not_analyzed",
		"analyzed": "english"
	}
}
```

当首次创建一个索引时，可以指定类型的映射。使用 _mapping 为新类型（存在的类型更新）增加映射，但不能修改存在的域映射，如果一个域的映射已经存在，那么该域的数据可能已经被索引，如果修改可能导致索引数据出错。可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed

```json
PUT /{_index}
{
    // 6.0 需要指定 type 为 _doc
	"_doc": {
        // 7.0 直接使用 properties 属性
        "properties": {
            "tweet": {
          		"type": "text",
          		"analyzer": "english"
        	},
        	"date": {
          		"type": "date"
            },
        	"name": {
          		"type": "keyword"
        	},
        	"user_id": {
          		"type": "long"
        	}
      	}
    }
}
```

ES 支持数组域（数组中所有值必须是相同数据类型），对于数组，没有特殊的映射需求，任何域都可以包含多个值，就像全文域分析得到多个词条。通过索引数组来创建新的域，ES 会用数组中第一个值的数据类型作为这个域的类型，当从 ES 得到一个文档，每个数组的顺序和当初索引文档时一样。数组是多值域索引的，可以搜索。但是无序的。

null 值为空域，不会被索引，包含 null，空数组，[null]

支持嵌套对象（内部对象），ES 会动态监测新的对象域并映射它们为对象，内部对象数组会被

##### 文档

最顶层或根对象，这个根对象被序列化成 JSON 并存储到 Elasticsearch 中。指定了唯一 ID。字段的名字可以是任何合法的字符串，但不可以包含英文句号 .

###### 索引文档

```json
PUT /{index}/{_doc}/{id}
{
	"filed": "value"
}
// 不存在才索引文档,成功 201 和元数据，存在的响应 409 Confilct 及错误
PUT /{index}/{_doc}/{id}?op_type=create
// 不存在才创建
PUT /{index}/{_doc}/{id}/_create
// 使用外部版本号索引文档
PUT /website/blog/2?version=5&version_type=external
{
  "title": "My first external blog entry",
  "text":  "Starting to get the hang of this..."
}
```

新建、索引、删除请求都是写操作，必须在主分片上完成之后才能被复制到相关的副本分片。流程为：

1.  客户端向 Node1 发生写操作请求
2.  节点使用文档 id 确定文档属于的分片。请求转发到对应分片所在的 Node3
3.  在 Node3 上执行请求。成功将请求转发到 Node1、Node2 的副本分片上，一旦所有的副本分片都报告成功，Node3 将向协调节点报告成功，协调节点向客户端报告成功
4.  客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成

以下参数可以控制写操作的过程：

*   consistency

    默认设置下，写操作前，主分片会要求必须有（`(int(primary + number_of_replicas) / 2) + 1`，如果当前索引有三个副本分片，则（primary + 3 replicas）/ 2）+ 1 = 3）的分片副本（可以是主分片或副本分片）处于活跃可用状态，才会执行写操作。

    新索引默认有 1 个副本分片，满足规定数量应该需要两个活动的分片副本。这些默认的设置会阻止在单一节点上操作，为了规避这个问题，只有当 number_of_replicas 大于 1 时，规定数量才会执行

*   timeout

    默认情况下，最多等待 1 分钟。100 是 100 毫秒，30s 是 30 秒

局部更新文档流程：

1.  客户端向 node 1 发送更新请求
2.  它将请求转发到主分片所在的 Node 3
3.  Node 3 从主分片检索文档，修改 _source 字段中的 JSON，并且尝试重新索引主分片的文档。如果文档已经被另一个进程修改，它会进行重试，超过 retry_on_conflict 次后放弃
4.  如果 Node 3 成功更新文档，它将新版本的文档并行转发到 Node1 和 Node2 上的副本分片，重新建立索引。一旦所有副本分片都返回成功，Node 3 向协调节点也返回成功，协调节点向客户端返回成功。

当主分片把更改转发到副本分片时，它不会转发更新请求。会转发完整文档的新版本。这些更改将会异步转发到副本分片，并且不能保证更新顺序。

###### 多文档模式

mget/bulk 模式类似于单文档模式，区别在于协调节点知道每个文档存在于那个分片中。它将整个请求分解成每个分片的多文档请求，并且将这些请求转发到每个参与节点。协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。

*   获取流程
    1.  客户端向 Node 1 发送 mget 请求
    2.  Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或副本分片的节点上。一旦收到所有答复，Node 1 构建响应并将其返回客户端
*   批量更新流程
    1.  客户端向 Node 1 发送 bulk 请求
    2.  Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机
    3.  主分片按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回客户端

###### 获取文档

```
GET /{index}/{_doc}/{id}?pretty
// 仅获取 _source 字段,即仅返回原始数据
GET /{index}/{_doc}/{id}/_source
```

可以从主分片或其他任意副本分片检索文档。流程为：

1.  客户端向 Node 1 发送获取请求
2.  节点使用文档的 _id 来确定文档属于分片 0。分片 0 的副本分片存在于所有的三个节点上。在这种情况下，它将请求转发到 Node 2
3.  Node 2 将文档返回给 Node 1，然后将文档返回给客户端

在处理读取请求时，协调节点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。在文档被检索时，已经被索引的文档可能已经存在于主分片上但还没有复制到副本分片。此时，副本分片可能会报告文档不存在，但主分片可能成功返回文档。一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的

使用 HEAD 方法获取文档是否存在，没有返回体，只返回一个请求报头

###### API

| METHOD |                             URL                              |        PARAMS        |      含义      |
| :----: | :----------------------------------------------------------: | :------------------: | :------------: |
|  PUT   | `/{index}/_doc/{id}` | `/{index}/_doc/{id}/_create` | `/{index}/_doc/{id}?op_type=create` | `{"field": "value"}` | 索引或更新文档 |
|  POST  |          `/{index}/_doc` | `/{index}/_create/{id}`           | `{"field": "value"}` |    索引文档    |
| DELETE |                     `{index}/_doc/{id}`                      |                      |    删除文档    |

###### 元数据

文档包含数据和元数据

*   _index

    索引是逻辑上的命名空间，由一个或多个分片组合在一起。必须小写，不能以下划线开头，不能包含逗号。文档位于一个索引内

*   _id

    字符串，在索引中标识文档，可以自己提供 _id 或自动生成 id（生成 ID 是 URL-safe，基于 Base64 编码且长度为 20 个字符的 GUID 字符串）

*   _version

    指定文档版本号，创建、修改、删除都会更新版本号。ES 使用 _version 号确定变更以正确顺序得到执行。如果旧版本的稳定在新版本之后到达，它可以被简单的忽略。

    一个场景是使用其他数据库作为主要的数据源，使用 ES 做数据检索，主数据库的所有更改发生时都需要被复制 ES，多进程处理同步时，如果主数据库已有版本号或能作为版本号的字段值。可以在 ES 中通过增加 `version_type=external` 到查询字符串的方式重用这些版本号（版本号必须大于零且小于 Java 的 LONG 最大正值）。ES 检查当前 _version 是否小于指定的版本号，如果请求成功，外部的版本号作为文档的新 _version 进行存储

###### 文档更新

文档是不可变的，它们不能被修改，只能被替换。如果想要更新现有的文档，需要重建索引或进行替换。在内部，ES 将旧文档标记为已删除，并增加一个全新的文档。旧文档不会立即消失，ES 会在后台清理已删除的文档

外部来看，在一个文档的某个位置进行部分更新，在内部处理流程为：检索-修改-重建索引。区别在与这个过程发生在分片内部，避免多次请求的网络开销。

*   使用 doc 参数和 _update 部分更新文档

    ```json
    POST /{index}/_doc/{id}/_update
    {
       "doc" : {
          "tags" : [ "testing" ],
          "views": 0
       }
    }
    ```

* 使用 upsert 更新的文档可能尚不存在

  ```json
  POST /{index}/_doc/{id}/_update?retry_on_conflict=5 // 指定重试次数
  {
  	"script": "ctx._source.views+=1",
  	"upsert": {
  		"views": 1
  	}
  }
  ```

###### 删除文档

Es 会在后台清理

```
DELETE /{index}/{_doc}/{id}
```

###### 乐观并发控制

当文档被修改时版本号递增。ES 使用这个 _version 号来确保变更以正确顺序得到执行，如果旧版本的文档在新版本之后到达，它可以被简单的忽略。

###### 批量操作文档

协调节点知道每个文档存在于那个分片中。它将多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点，协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。

*   使用 `_mget` API 获取一个或多个文档，可以指定索引执行

    ```json
    GET /_mget
    {
      "docs": [
        {
          "_index": "website",  // 必须
          "_id": 3,    // 必须
          "_source": ["title", "text"] // 参数字段
        },
        {
          "_index": "website",
          "_id": 1
        }
      ]
    }
    ```

*   `_bulk` API 允许在单个步骤中进行多次 create、index、update、delete。请求体类型一个有效的单行 JSON 文档流，通过换行符 \n 连接到一起。每行要以换行符结尾，包括最后一行，不能包含未转义的换行符。

##### 搜索操作

搜索可以做到结构化查询，使用排序。全文检索（找出所有匹配关键字并按照相关性排序后返回结果）。ES 搜索相关概念：

*   映射（Mapping）

    描述数据在每个字段内如何存储

*   分析（Analysis）

    全文是如何处理使之可以被搜索的

*   领域特定查询语言（Query DSL）

*返回字段*

|   字段    |                             描述                             |
| :-------: | :----------------------------------------------------------: |
|   took    |                          耗时，毫秒                          |
| timed_out |                       指示查询是否超时                       |
|  shards   |        查询访问的分片总数，以及这些分片成功失败情况。        |
|   hits    | 查询结果，包含 total 字段指定查询结果，max_score 最大评分、hits 结果文档数组 |

###### 搜索 API

在查询字符串（url 编码）中传递所有的参数或使用请求体进行查询（使用 JSON 查询表达式）。查询字符串搜索允许任何用户在索引任意字段上执行可能较慢且重量级的查询，甚至将集群拖垮。在生产环境中更多使用功能全面的 request body 查询 API。通常规则是，使用查询语句来进行全文搜索或相关性的搜索，除此之外的情况都使用过滤

*   查询字符串

    ```html
    # + 前缀表示必须与查询条件匹配。- 前缀表示一定不与查询条件匹配，没有 +/- 的所有条件都是可选的，匹配越多，文档越相关
    GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary
    # name 字段中包含 mary 或 john，date 值大于 2014-09-10 _all 字段包含 aggregations 或 geo
    +name:(mary john) +date:>2014-09-10 +(aggregations geo)
    # 上述 url 编码
    ?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)
    ```


| method |                   url                   |    param    |         描述         |
| :----: | :-------------------------------------: | :---------: | :------------------: |
|  GET   |               `/_search`                |             | 空搜索，返回所有文档 |
|  GET   | `/{index}/_doc/_validate/query?explain` | 请求体 json |   验证查询是否合法   |
|        |                                         |             |                      |
|        |                                         |             |                      |

*   _all 字段

    当索引一个文档的时候，Es 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引，除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索（当 _all 字段不再有用时，可以将它设置为失效）

###### 过滤情况

查询被设置成一个不评分或过滤的查询，即只查看查询是否匹配。结果会被缓存以便快速读取，通常情况下过滤性能会更好（理论上非评分查询先于评分查询执行）。bool 查询使用过滤器 filter 过滤。在内部，ES 对非评分的查询会执行多个操作：

1. 查找匹配文档
2. 创建 bitset（过滤器会创建一个包含 0 和 1 的数组）
3. 迭代 bitset，查询生成 bitset 后，es 会循环迭代 bitset 找出满足所有过滤条件的匹配文档的集合
4. 增量使用计数，ES 能够缓存非评分查询（ES 会为每个索引跟踪保留使用的历史状态，如果查询在最近的 256 次查询中会被用到，它就会被缓存到内存中，当 bitset 被缓存后，缓存会在那些低于 10000 个文档或少于 3% 的总索引数的段中被忽略）

```json
// range 指定区间内的数字、时间、字符串(字典排序或字母排序)：gt：大于、gte：大于等于、lt：小于、lte：小于等于，支持单区间范围和日期范围
[
    "range": {
        "_field": {
            "gte": 20,
            "lt": 30
        }
    },
    "range": {
    	// 支持日期计算 now-1h 过去一小时，2020-12-01 00:00:00||+1M 2020-12-01 日加 1 月
        "timestamp_field": {
            "gt": "YYYY-MM-DD HH:i:s"
            "lt": "2014-01-17 12:23:32"
        }
	}
]
// term 查询被用于精确值（数字、时间、布尔、not_analyzed 索引的文本）匹配，包含在大小写、重音、空格等方面差异，不分析文本
{
    "term": {"_field": search_value}
}
// terms 允许指定多值进行匹配，如果这个字段包含了指定值中任何一个值，这个文档满足条件
{
    "terms": {"_field": ["value1", "value2", "value3", "value4"]}
}
// exists 查询 missing 查询被用于查找指定字段中有值（exists）或无值（missing）的文档。类似 IS_NULL 或 NOT_IS_NULL，用于查询某个字段是否有值
{
    "exists": {
        "field": "_field"
    }
}
// 对过滤可以使用 constant_score 来包裹
{
    "query" : {
        "constant_score" : {
            "filter" : {
                "term" : {
                    "_field" : "value"
                }
            }
        }
    }
}
// 等价 bool 筛选,bool 过滤器支持 must、must_not、should 子语句
{
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "should": [
                        {
                            "term": {"_field": "value"}
                        }
                    ]
                }
            }
        }
    }
}
// 子语句支持嵌套 bool 查询
{
   "query" : {
      "filtered" : {
         "filter" : {
            "bool" : {
              // term 和 bool 查询二者满足之一即可
              "should" : [
                { "term" : {"productID" : "KDKE-B-9947-#kL5"}}, 
         		// 嵌套 bool 查询
                { "bool" : { 
                  "must" : [
                    { "term" : {"productID" : "JODL-X-1937-#pV7"}}, 
                    { "term" : {"price" : 30}} 
                  ]
                }}
              ]
           }
         }
      }
   }
}
```

* term 与 terms 是包含操作，而非等值操作。即 term 字段如果包含匹配值及其他值也会被包含 bitset。如果期望得到精确相等的结果，最好增加一个索引字段这个字段存储 term 字段包含词项的数量，查询时同时匹配词的数量为 1

  ```json
  {
      "must": [
          {"term": {"_field": "value"}},
          {"term": {"_term_field_count": 1}}
      ]
  }
  ```

* range 查询字符串时实际是在为范围内的每个词项都执行 term 过滤器，会比日期或数字的范围过滤慢许多。在过滤低基数（只有少量唯一词）时可以正常工作，唯一词项越多，字符串范围的计算会越慢

###### 查询情况

查询变成了一个评分查询，不但会判断这个文档是否匹配，还会根据匹配程度进行评分（评分查询计算每一个文档与此查询相关程度计算方式可能是 TF/IDF、地理位置邻近、模糊相似等，同时将这个相关程度分配给表示相关性的字段 _score，并按照相关性进行排序），查询结果并不缓存。默认按照相关性得分排序，即每个文档跟查询的匹配程度。

bool 查询会为每个文档计算相关度评分，再将所有匹配的 must 和 should 语句的分数 _score 求和，最后除以 must 和 should 语句的总数。must_not 不影响评分。默认情况下，没有 should 语句是必须匹配的，当没有 must 语句时，至少有一个 should 语句必须匹配。可以使用 minimum_should_match 参数控制需要匹配的 should 语句数量（支持参数和百分百）

```json
GET /_index/_doc/_search
{
    "query": {
        // 在全文字段上使用 match 查询，在执行查询前，它将分析查询字符串，在精确值字段（或 not_analyzed）使用会精确匹配给定值
        "match": {
            "_field": "keyword"
        }
    },
  	// 获取特定字段
  	"_source": ["filed", "filed1"]
}
// 匹配所有文档，未指定查询方式时，它是默认的查询
{"match_all": {}}
// 在多个字段上执行相同的 match 查询
{
    "multi_match": {
        "query": "search text",
        "fields": ["_filed", "_field1", "_filed2"]
    }
}
// 精确匹配一系列单词或短语
{
	"query": {
		"match_phrase": {
			"_field": "keyword phrase"
		}
	}
}
// 以下查询等价
{
    "match": {"_field": "word word1"}
}
{
    "bool": {
        "should": [
    	    {"term": {"_field": "word"}},
		    {"term": {"_field": "word1"}}
    	]
    }
}
// 以下查询等价
{
    "match": {
        "_field": {
            "query": "word word1",
            "operator": "and"
        }
    }
}
{
    "bool": {
        "must": [
            {"term": {"_field": "word"}},
            {"term": {"_field": "word1"}}
        ]
    }
}
// 以下查询等价
{
    "match": {
        "_field": {
            "query": "word word1 word2",
            "minimum_should_match": "75%"
        }
    }
}
{
    "bool": {
        "should": [
            {"term": {"_field": "word"}},
            {"term": {"_field": "word1"}},
            {"term": {"_field": "word2"}}
        ],
        "minimum_should_match": "75%"
    }
}
```

* match 是一个高级全文查询，既能处理全文字段，也能处理精确字段。ES 执行 match 步骤：

  1. 检查字段类型
  2. 分析查询字符串，单项词：底层使用单个 term 查询，多项词：底层执行多个 term 并合并（转换成 bool）
  3. 查找匹配文档
  4. 为每个文档评分：结合词频（term frequency：词在相关文档对应字段中出现的频率）、反向文档频率（inverse document frequency：词在所有文档的对应字段中出现的频率）、字段的长度（字段越短相关度越高）

  查询多个词时，可以使用 and 语义来使文档必须同时包含多个词而非单个匹配，或使用 minimum_should_match 设置匹配程度

  

###### 高亮搜索

使用 hightlight 参数指定，返回匹配部分，以 `<em>`  标签封装

```
GET /_index/_doc/_search
{
	"query": {
		"match_phrase": {
			"_field": "keyword phrase"
		}
	},
	"highlight": {
		"fields": {
			"_field": {}
		}
	}
}
```

###### 分页

```
GET /_search?size=5;
GET /_search?size=5&from=10
GET /_search
{
	"from": 30,
	"size": 10
}
```

ES 接受 from（显示应该跳过的初始结果数量，默认是 0）和 size（显示应该返回的结果数量，默认是 10）参数。结果集在返回之前先进行排序（一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的）因此应该避免深度分页（web 搜索引擎对任何查询都不要返回超过 1000 个）

*   分布式系统中的深度分页

    假设在一个有 5 个主分片的索引中搜索，当请求结果的第一页（1～10），每个分片产生前 10 的结果，并且返回给协调节点。协调节点对 50 个结果排序得到全部结果的前 10。在深度分页的情况下，请求第 1000 页（10001～10010），每个分片产生前 10010 结果，然后协调节点对全部 50050 个结果排序最后丢弃结果中的 50040 个结果。在分布式系统中，对结果排序的成本随分页的深度成指数上升。

可以使用游标查询来执行大批量的文档查询，避免深度分页的代价。游标查询会先做查询初始化，然后再批量拉取结果。 游标查询会取某个时间点的快照数据。查询初始化后索引上的变化会被忽略。

```
// 指定保持游标查询窗口 1m
GET /{index}/_search?scroll=1m
{
	”query": {
		"match_all": {}
	},
	"sort": {
		"_doc"
	},
	"size": 1000 
}
```

查询的返回结果包含 `_scroll_id`（base64 编码）标识游标

```
// 使用 scroll_id 请求同一游标数据
GET /_search/scorll
{
	"scorll": "1m",   // 再次设置游标查询过期时间为 1m
	"scroll_id": "base64_string"
}
```



###### 排序

默认情况下，返回结果按照相关性（_score 字段）进行排序（最相关的文档拍照最前）

* 按照字段值排序，返回结果中会包含一个 sort 元素，包含用于排序的值，默认升序排序

  ```json
  {
  	"query": {
  		"bool": {}
  	},
  	"sort": {
  		"_field": {
  			"order": "desc/asc "
  		}
  	}
  }
  // 简写
  {
  	”sort": "field"
  }
  // 多级排序，按照声明的先后顺序排序，首先按第一个条件排序，仅当结果集第一个 sort 值完全相同时才会按照第二个条件进行排序
  {
  	"query": {"bool": {}}
  	"sort": [
  		{"_field": {"order": "desc"}},
  		{"_score": {"order": "desc"}}
  	]
  }
  // 多值字段排序,字段有多个值的排序（这些值没有固有的顺序，一个多值的字段仅仅是个多个值的包装），对于数字和日期，可以将多值字段减为单值，通过 min、max、avg、sum 排序模式
  {
      "sort": {
          "order": "asc",
          "mode": "min"
      }
  }
  ```

###### 请求体查询

```json
// 单查询
{
	"QUERY_NAME": {
        "FIELD_NAME": {
            "ARGUMENT": "VALUE"
        }
    }
}
// 复合查询
{
    "bool": {
        "must":     { "match": { "tweet": "elasticsearch" }},
        "must_not": { "match": { "name":  "mary" }},
        "should":   { "match": { "tweet": "full text" }},
        "filter":   { "range": { "age" : { "gt" : 30 }} }
    }
}
```

查询语句可以组合成复杂的查询：

* 叶子语句（操作语句，类似 match）被用于将查询字符串和一个字段（或多个字段）对比
* 复合语句：主要用于合并其他查询语句（bool 语句允许在需要时组合其他语句，包含：must、must_not、should、filters）支持嵌套，一条复合语句可以将多条语句（叶子和复合语句）合并成一个单一的查询语句

布尔查询用于组合查询语句。将多个子查询组合在一起，每个子查询都独自计算文档的相关性得分，一旦得分被计算出来，bool 查询就将这些得分进行合并并返回一个代表整个布尔操作的得分。支持以下组合语句（如果没有 must 语句，至少需要能够匹配一条 should 语句，如果存在至少一条 must 语句，对 should 语语句的匹配没有要求）：

* must

  文档必须匹配这些条件才能被包含进来

* must_not

  文档必须不匹配这些条件才能被包含进来

* should

  如果满足这些语句中的任意语句，将增加 _score，否则，无任何影响，主要用于修正每个文档的相关性得分

* filter

  必须匹配，但以不评分，过滤模式来进行。对评分没有贡献，只是根据过滤标准来排除或包含文档

###### 相关性

ES 的相似度算法被定义为检索词频率/反向文档频率，TF/IDF，包含：

* 检索词频率

  检索词在该字段出现的频率，出现频率越高，相关性也越高

* 反向文档频率

  每个检索词在索引中出现的频率，频率越高，相关性越低，检索词出现在多数文档中会比出现在少数文档中的权重更低

* 字段长度准则

  字段越长，相关性越低，检索词出现在一个短的 title 要比同样的此出现在一个长的 content 字段权重大

单个查询可以联合使用 TF/IDF 和其他方式（短语查询中检索词的距离和模糊查询里的检索词相似度）如果多条查询子句被合并为一条复合查询语句，则每个查询子句计算得出的评分会被合并到总的相关性评分中

```json
// explain 设为 true 获取 _score 评分依据
GET /{index}/_doc/_search?explain=true
{
	"query": {
		"match": {
			"field": "value"
		}
	}
}
// 获取某个文档为何会被匹配
GET /{index}/_doc/{_id}/_explain
```

##### 索引管理

通过索引文档的方式创建的索引，采用默认配置，字段通过动态映射的方式被添加到类型映射。当需要定制索引属性时可以手动创建索引。和使用索引模板。

```yaml
action.auto_create_index: false # 禁止自动创建索引
action.destructive_requires_name: true # 只允许删除特定名称索引，不允许通配符索引删除
```

###### 索引设置

```json
// 创建时设置索引
PUT /{index}
{
    "settings": {
        "number_of_shards": 1, // 索引主分片数，默认 5，索引创建后不能更改
        "number_of_replicas": 0, // 每个主分片的副本数，默认 1，对于活动的索引库，可以随时更改
        // 指定自定义分析器
        "analysis": {
            "char_filter": {
                "&_to_and": {
                    "type":       "mapping",
                    "mappings": [ "&=> and "]
            }},
            "filter": {
                "my_stopwords": {
                    "type":       "stop",
                    "stopwords": [ "the", "a" ]
            }},
            "analyzer": {
                "my_analyzer": {
                    "type":         "custom",
                    "char_filter":  [ "html_strip", "&_to_and" ],
                    "tokenizer":    "standard",
                    "filter":       [ "lowercase", "my_stopwords" ]
            }}
    },
    "mappings": {
      // 如果遇到新字段，抛出异常
      "_doc": {
        "dynamic": "strict",
        "properties": {
          "field": {"type": "text"},
          "filed1": {
            // 遇到新字段，动态创建新字段
            "type": "object",
            "dynamic": true
          }
        }
      }
    }
      
}
// 删除索引
DELETE /{index}
DELETE /{index_mattern}
// 修改索引配置
PUT /{index}/_settings
{
    "{arguments}": "value"
}
```

可以用 dynamic 配置控制动态映射行为支持：true（动态添加新的字段-缺省时的值）、false（忽略新的字段，不会改变 _source 字段内容，仍然包含被索引的整个 JSON 文档，新的字段不会被加到映射中也不可搜索）、strict（如果遇到新字段抛出异常）

###### 动态模版

使用 `dynamic_templates` 可以控制新检测生成字段的映射。每个模版都有一个名称，模版按照顺序来检测，第一个匹配的模版会被启用

```json
PUT /{index}
{
	"mappings": {
		"_doc": {
      // 指定缺省设置
      "_default_": {
        "_all": {"enabled": false}
      }
			"dynamic_templates": {
				{
					"es": {
						"match": "*_es",  // 匹配字段名以 _es 结尾的字段
						"match_mapping_type": "string",   // 允许应用模版到特定类型的字段上。
						"mapping": {
							"type": "string",
							"analyzer": "english"
						}
					}
				}
			}
		}
	}
}
```

###### 索引别名

索引别名类似软连接，可以指向一个多个索引。可以给任何一个需要索引名的 API 来使用。

| 请求方式 |               URL               |           作用           |
| :------: | :-----------------------------: | :----------------------: |
|   PUT    | `/{index}/_alias/{index_alias}` |  设置索引别名指向 index  |
|   GET    |       `/*/_alias/{index}`       | 检测这个别名指向那个索引 |
|   GET    |       `/{index}/_alias/*`       |   那些别名指向这个索引   |

一个别名可以指向多个索引，在添加别名到新索引的同时必须从旧的索引中删除它。这个操作需要原子化

```json
POST /_aliases
{
    "actions": [
        { "remove": { "index": "my_index_v1", "alias": "my_index" }},
        { "add":    { "index": "my_index_v2", "alias": "my_index" }}
    ]
}
```



##### 聚合

aggregations 基于数据生成一些精细的分析结果，聚合与 SQL 中的 GROUP BY 类似

```
GET /_index/_doc/_search
{
	"aggs": {
		"all_intere"
	}
}
```

