### Elasticsearch

#### 集群

集群由一个或多个拥有相同 cluster.name 配置的节点组成，它们共同承担数据和负载压力。当有节点加入集群或从集群中移除节点时，集群将会重新平均分布所有的数据

##### API

###### 集群

| method |         url          |            params             |    作用    |
| :----: | :------------------: | :---------------------------: | :--------: |
|  GET   |  `/_cluster/health`  |                               |  集群健康  |
|  PUT   | `/{index}/_settings` | {"number_of_replicas": {num}} | 修改副本数 |
|        |                      |                               |            |

##### 节点

###### 主节点

当一个节点（运行中的 Elasticsearch 实例）被选举成主节点时，它将负责管理集群范围内的所有变更（索引的增加、删除，节点的增加、删除），而不涉及到文档级别的变更和搜索等操作

客户端可以将请求发送到集群中的任何节点，包括主节点。每个节点都知道任意文档所处的位置，将请求直接转发到存储文档的节点。并将结果返回（直接在服务端完成转发）

##### 分布式特性

屏蔽了分布式系统的复杂性

-   分配文档到不同的容器 或 *分片* 中，文档可以储存在一个或多个节点中
-   按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡
-   复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失
-   将集群中任一节点的请求路由到存有相关数据的节点
-   集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复

###### 分片

一个 *分片* 是一个底层的 *工作单元* ，它仅保存了全部数据中的一部分。一个分片是一个 Lucene 的实例，它本身就是一个完整的搜索引擎。 文档被存储和索引到分片内，应用程序是直接与索引而不是与分片进行交互

Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里

一个分片可以是：

*    *主* 分片

     索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量（技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长）

     路由一个文档到一个分片时：shard = hash(routing) % number_of_primary_shards。所有的文档 API（get、index、delete、bulk、update、mget）都可以接受 routing 路由参数，通过这个参数可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档都被存储到同一个分片中。

*   副本分片

    是主分片的拷贝，硬件故障的冗余备份，并提供搜索和返回文档等读操作服务，节点丢失后，其上主分片在其他节点的副分片会提升为主分片

在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改（在相同节点数目的集群上增加更多的副本分片不能提高性能）

#### 数据

在 Elasticsearch 中，每个字段的所有数据都是默认被索引的。即每个字段都有为了快速检索设置的专用倒排索引。它能在同一个查询中使用所有这些倒排索引。

Es 中数据可以分为：

*   精确值

    确定的值（日期，ID），字符串也可以表示精确值，精确值很容易查询，结果是二进制的：要么匹配，要么不匹配。

*   全文

    在于该文档匹配查询的程度有多大，即相关性。

##### 分析

###### 倒排索引

Es 使用倒排索引的结构，适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有 一个包含它的文档列表

为了创建倒排索引，首先会将每个文档拆分成单独的词（词条或 tokens），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在那个文档。只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式

###### 分析

分析将一个全文域分成适合倒排索引的独立词条，之后将这些词条统一化为标准格式以提高搜索性（recall）。分析器首先将字符串按顺序通过每个字符过滤器（一个字符过滤器可以用来去掉 HTML，或将 & 转化成 and），在分词前整理字符串。其次字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点时，可能会将文本拆分成词条，最后词条按顺序通过每个 token 过滤器，这个过程可能会改变词条，删除词条，增加词条。Es 内置了开箱即用的字符过滤器、分词器、token 过滤器

###### 内置分析器

索引文档时，它的全文域被分析成词条以用来创建倒排索引。全文域搜索时，需要将查询字符串通过相同的分析过程，以保证搜索的词条格式与索引中的词条格式一致。查询精确域时不会分析查询字符串，而是搜索指定的精确值

*   标准分析器

    默认使用分析器。根据 Unicode 定义的单词边界划分文本。删除大部分标点，将词条小写。当 Es 在文档中检测到一个新的字符串域时，它会自动设置其为一个全文字符串域，使用标准分析器进行分析

*   简单分析器

    在任何不是字母的地方分隔文本，将词条小写

*   空格分析器

    在空格的地方划分文本

*   语言分析器

    特定语言分析器可以考虑指定语言的特定（英语分析器附带了一组英语无用词：and the，它们会被删除，可以提取单词的词干）

###### Analyze API 

查看文本分析

```json
GET /_analyze
{
  "analyzer": "standard",
  "text": "Text to analyze"
}
{
	"tokens": [
      {
         "token":        "text",
         "start_offset": 0,
         "end_offset":   4,
         "type":         "<ALPHANUM>",
         "position":     1
      },
      {
         "token":        "to",
         "start_offset": 5,
         "end_offset":   7,
         "type":         "<ALPHANUM>",
         "position":     2
      },
      {
         "token":        "analyze",
         "start_offset": 8,
         "end_offset":   15,
         "type":         "<ALPHANUM>",
         "position":     3
      }
   ]
}
```

*   token 是实际存储到索引中的词条
*   position 指词条在原始文本中出现的位置
*   start_offset/end_offset 指明字符在原始字符串中位置
*   type 为每个分析器独有

###### 映射

| method |        url        |       描述       |
| :----: | :---------------: | :--------------: |
|  GET   | /{index}/_mapping | 获取文档字段映射 |

##### 文档

最顶层或根对象，这个根对象被序列化成 JSON 并存储到 Elasticsearch 中。指定了唯一 ID。字段的名字可以是任何合法的字符串，但不可以包含英文句号 .

###### 索引文档

新建、索引、删除请求都是写操作，必须在主分片上完成之后才能被复制到相关的副本分片。流程为：

1.  客户端向 Node1 发生写操作请求
2.  节点使用文档 id 确定文档属于的分片。请求转发到对应分片所在的 Node3
3.  在 Node3 上执行请求。成功将请求转发到 Node1、Node2 的副本分片上，一旦所有的副本分片都报告成功，Node3 将向协调节点报告成功，协调节点向客户端报告成功
4.  客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成

以下参数可以控制写操作的过程：

*   consistency

    默认设置下，写操作前，主分片会要求必须有（`(int(primary + number_of_replicas) / 2) + 1`，如果当前索引有三个副本分片，则（primary + 3 replicas）/ 2）+ 1 = 3）的分片副本（可以是主分片或副本分片）处于活跃可用状态，才会执行写操作。

    新索引默认有 1 个副本分片，满足规定数量应该需要两个活动的分片副本。这些默认的设置会阻止在单一节点上操作，为了规避这个问题，只有当 number_of_replicas 大于 1 时，规定数量才会执行

*   timeout

    默认情况下，最多等待 1 分钟。100 是 100 毫秒，30s 是 30 秒

局部更新文档流程：

1.  客户端向 node 1 发送更新请求
2.  它将请求转发到主分片所在的 Node 3
3.  Node 3 从主分片检索文档，修改 _source 字段中的 JSON，并且尝试重新索引主分片的文档。如果文档已经被另一个进程修改，它会进行重试，超过 retry_on_conflict 次后放弃
4.  如果 Node 3 成功更新文档，它将新版本的文档并行转发到 Node1 和 Node2 上的副本分片，重新建立索引。一旦所有副本分片都返回成功，Node 3 向协调节点也返回成功，协调节点向客户端返回成功。

当主分片把更改转发到副本分片时，它不会转发更新请求。会转发完整文档的新版本。这些更改将会异步转发到副本分片，并且不能保证更新顺序。

###### 多文档模式

mget/bulk 模式类似于单文档模式，区别在于协调节点知道每个文档存在于那个分片中。它将整个请求分解成每个分片的多文档请求，并且将这些请求转发到每个参与节点。协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。

*   获取流程
    1.  客户端向 Node 1 发送 mget 请求
    2.  Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或副本分片的节点上。一旦收到所有答复，Node 1 构建响应并将其返回客户端
*   批量更新流程
    1.  客户端向 Node 1 发送 bulk 请求
    2.  Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机
    3.  主分片按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回客户端

###### 获取文档

可以从主分片或其他任意副本分片检索文档。流程为：

1.  客户端向 Node 1 发送获取请求
2.  节点使用文档的 _id 来确定文档属于分片 0。分片 0 的副本分片存在于所有的三个节点上。在这种情况下，它将请求转发到 Node 2
3.  Node 2 将文档返回给 Node 1，然后将文档返回给客户端

在处理读取请求时，协调节点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。在文档被检索时，已经被索引的文档可能已经存在于主分片上但还没有复制到副本分片。此时，副本分片可能会报告文档不存在，但主分片可能成功返回文档。一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的

###### API

| METHOD |                             URL                              |        PARAMS        |      含义      |
| :----: | :----------------------------------------------------------: | :------------------: | :------------: |
|  PUT   | `/{index}/_doc/{id}` | `/{index}/_doc/{id}/_create` | `/{index}/_doc/{id}?op_type=create` | `{"field": "value"}` | 索引或更新文档 |
|  POST  |          `/{index}/_doc` | `/{index}/_create/{id}`           | `{"field": "value"}` |    索引文档    |
| DELETE |                     `{index}/_doc/{id}`                      |                      |    删除文档    |

###### 元数据

文档包含数据和元数据

*   _index

    索引是逻辑上的命名空间，由一个或多个分片组合在一起。必须小写，不能以下划线开头，不能包含逗号。文档位于一个索引内

*   _id

    字符串，在索引中标识文档，可以自己提供 _id 或自动生成 id（生成 ID 是 URL-safe，基于 Base64 编码且长度为 20 个字符的 GUID 字符串）

*   _version

    指定文档版本号，创建、修改、删除都会更新版本号。ES 使用 _version 号确定变更以正确顺序得到执行。如果旧版本的稳定在新版本之后到达，它可以被简单的忽略。

    一个场景是使用其他数据库作为主要的数据源，使用 ES 做数据检索，主数据库的所有更改发生时都需要被复制 ES，多进程处理同步时，如果主数据库已有版本号或能作为版本号的字段值。可以在 ES 中通过增加 `version_type=external` 到查询字符串的方式重用这些版本号（版本号必须大于零且小于 Java 的 LONG 最大正值）。ES 检查当前 _version 是否小于指定的版本号，如果请求成功，外部的版本号作为文档的新 _version 进行存储

##### 文档操作

###### 使用外部版本号索引文档

```json
PUT /website/blog/2?version=5&version_type=external
{
  "title": "My first external blog entry",
  "text":  "Starting to get the hang of this..."
}
```

###### 文档部分更新

文档是不可变的，它们不能被修改，只能被替换。外部来看，在一个文档的某个位置进行部分更新，在内部处理流程为：检索-修改-重建索引。区别在与这个过程发生在分片内部，避免多次请求的网络开销。

*   使用 doc 参数

    ```json
    POST /website/blog/1/_update
    {
       "doc" : {
          "tags" : [ "testing" ],
          "views": 0
       }
    }
    ```

###### 批量操作文档

*   使用 `_mget` API 获取一个或多个文档，可以指定索引执行

    ```json
    GET /_mget
    {
      "docs": [
        {
          "_index": "website",  // 必须
          "_id": 3,    // 必须
          "_source": ["title", "text"] // 参数字段
        },
        {
          "_index": "website",
          "_id": 1
        }
      ]
    }
    ```

*   `_bulk` API 允许在单个步骤中进行多次 create、index、update、delete。请求体类型一个有效的单行 JSON 文档流，通过换行符 \n 连接到一起。每行要以换行符结尾，包括最后一行，不能包含未转义的换行符。

##### 搜索操作

搜索可以做到结构化查询，使用排序。全文检索（找出所有匹配关键字并按照相关性排序后返回结果）。ES 搜索相关概念：

*   映射（Mapping）

    描述数据在每个字段内如何存储

*   分析（Analysis）

    全文是如何处理使之可以被搜索的

*   领域特定查询语言（Query DSL）

*返回字段*

|   字段    |                             描述                             |
| :-------: | :----------------------------------------------------------: |
|   took    |                          耗时，毫秒                          |
| timed_out |                       指示查询是否超时                       |
|  shards   |        查询访问的分片总数，以及这些分片成功失败情况。        |
|   hits    | 查询结果，包含 total 字段指定查询结果，max_score 最大评分、hits 结果文档数组 |

###### 搜索 API

在查询字符串（url 编码）中传递所有的参数或使用请求体进行查询（使用 JSON 查询表达式）。查询字符串搜索允许任何用户在索引任意字段上执行可能较慢且重量级的查询，甚至将集群拖垮。在生产环境中更多使用功能全面的 request body 查询 API。

*   查询字符串

    ```html
    # + 前缀表示必须与查询条件匹配。- 前缀表示一定不与查询条件匹配，没有 +/- 的所有条件都是可选的，匹配越多，文档越相关
    GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary
    # name 字段中包含 mary 或 john，date 值大于 2014-09-10 _all 字段包含 aggregations 或 geo
    +name:(mary john) +date:>2014-09-10 +(aggregations geo)
    # 上述 url 编码
    ?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)
    ```

    

| method |    url     | param |         描述         |
| :----: | :--------: | :---: | :------------------: |
|  GET   | `/_search` |       | 空搜索，返回所有文档 |
|        |            |       |                      |
|        |            |       |                      |
|        |            |       |                      |

*   _all 字段

    当索引一个文档的时候，Es 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引，除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索（当 _all 字段不再有用时，可以将它设置为失效）

###### 分页

ES 接受 from（显示应该跳过的初始结果数量，默认是 0）和 size（显示应该返回的结果数量，默认是 10）参数。结果集在返回之前先进行排序（一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的）因此应该避免深度分页（web 搜索引擎对任何查询都不要返回超过 1000 个）

*   分布式系统中的深度分页

    假设在一个有 5 个主分片的索引中搜索，当请求结果的第一页（1～10），每个分片产生前 10 的结果，并且返回给协调节点。协调节点对 50 个结果排序得到全部结果的前 10。在深度分页的情况下，请求第 1000 页（10001～10010），每个分片产生前 10010 结果，然后协调节点对全部 50050 个结果排序最后丢弃结果中的 50040 个结果。在分布式系统中，对结果排序的成本随分页的深度成指数上升。