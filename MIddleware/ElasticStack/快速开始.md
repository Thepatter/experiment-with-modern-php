### 快速开始

#### 配置及目录结构

##### 配置文件

###### es 

*   内核参数设置

    ```shell
    sysctl -w vm.max_map_count=262144
    ```

*   配置文件

    ```shell
    # 允许自动创建 X-PACK 索引，* 允许自动创建所有索引的值
    vim elasticsearch.yml
    action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*
    ```

*   Systemd 下服务及运行

    ```shell
    # 服务自启动
    sudo /bin/systemctl daemon-reload
    sudo /bin/systemctl enable elasticsearch.service
    # 自动重启
    sudo systemctl edit elasticsearch.service
    [Service]
    Restart=always
    sudo systemctl daemon-reload
    # 服务启动
    sudo systemctl start elasticsearch.service
    ```

*   安装插件

    *   中文分词插件

        ```shell
        # 中文分词
        ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.4/elasticsearch-analysis-ik-6.2.4.zip
        ```

        支持自定义分词词典，UTF-8 编码，一行一个自定义分词，修改 */etc/elasticsearch/analysis-ik/IKAnalyzer.cfg.xml*  配置文件并重启

        ```xml
        <!-- 将自定义字典文件放入 /etc/elasticsearch/analysis-ik 文件夹，并修改权限为读写 -->
        <entry key="ext_dict">project.dic</entry>
        ```

    *   sql 查询插件

        ```shell
        ./bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.2.4.0/elasticsearch-sql-6.2.4.0.zip
        ```

        配置 web 站点

        下载 https://github.com/NLPchina/elasticsearch-sql/releases/download/5.4.1.0/es-sql-site-standalone.zip 并解压

        ```
        cd site-server
        npm install express --save
        node node-server.js
        ```

        修改 `elasticsearch.yml` 添加

        ```
        http.cors.enabled: true
        http.cors.allow-origin: "*"
        ```

        重启后访问，`http:ip:8080`，并修改顶部的地址为 elastic 地址

###### Logstash

配置文件位于 `/etc/logstash` 目录，包含以下配置文件

*   logstash.yml

    包含 Logstash 配置标志。可以在该文件中设置标志，而不是在命令行传递标志。在命令行中设置的任何标志都将覆盖文件中的相应设置

    *配置项*

    |              设置               |                             描述                             |                            默认值                            |
    | :-----------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
    |           `node.name`           |                       节点的描述性名称                       |                         机器的主机名                         |
    |           `path.data`           |          Logstash 及其插件用于任何持久性需求的目录           |                     `LOGSTASH_HOME/data`                     |
    |          `pipeline.id`          |                          管道的 ID                           |                            `main`                            |
    |       `pipeline.workers`        | 并行执行管道过滤器和输出阶段的工作人员数量。如果发现事件正在备份，或者CPU未饱和，请考虑增加此数量以更好地利用机器处理能力 |                     默认为 CPU 核心数据                      |
    |      `pipeline.batch.size`      | 在尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量大小通常效率更高，但会以增加的内存开销为代价。需要增加 `jvm.options` 配置文件中的 JVM 堆空间 |                             125                              |
    |     `pipeline.batch.delay`      | 在创建管道事件批次时，在将不足量的批次发送给管道工作人员之前，等待每个事件需要多长时间（毫秒） |                              50                              |
    |   `pipeline.unsafe_shutdown`    | 设置`true`为时，即使在内存中仍存在飞行中事件时，也会强制Logstash在关机期间退出。默认情况下，Logstash将拒绝退出，直到所有收到的事件都被推送到输出。启用此选项可能导致关机期间数据丢失。 |                            false                             |
    |          `path.config`          | 主管道的 `Logstash` 配置路径。 如果您指定目录或通配符，则会按字母顺序从目录中读取配置文件。 |                          特定于平台                          |
    |         `config.string`         | ‎一个字符串, 包含用于主管线的管线配置。使用与配置文件相同的语法。 |                             没有                             |
    |     `config.test_and_exit`      | 设置为时`true`，检查配置是否有效，然后退出。请注意，使用此设置不会检查grok图案的正确性。Logstash可以从目录读取多个配置文件。如果您将此设置与`log.level: debug`Logstash 结合使用，则Logstash会记录组合配置文件，并为每个配置块注释源自它的源文件 |                            false                             |
    |     `config.reload.automat`     | 设置 true 时，定期检查配置是否已更改，并在配置发生更改时重新加载配置。这也可以通过SIGHUP信号手动触发 |                            false                             |
    |     `config.reload.interva`     |             Logstash在几秒钟内检查配置文件的更改             |                              3s                              |
    |         `config.debug`          | 设置为时`true`，将完全编译的配置显示为调试日志消息。你还必须设置`log.level: debug`。警告：日志消息将包括任何以明文形式传递给插件配置的*密码*选项，并可能导致明文密码出现在您的日志中 |                            false                             |
    |    `config.support_escapes`     | 设置`true`为时，带引号的字符串将处理以下转义序列：`\n`成为文字换行符（ASCII 10）。`\r`成为一个文字回车（ASCII 13）。`\t`成为一个文字标签（ASCII 9）。`\\`成为文字反斜杠`\`。`\"`成为一个文字双引号。`\'`成为一个文字引号 |                            false                             |
    |            `modules`            |   配置时，`modules`必须位于本表上面描述的嵌套YAML结构中。    |                             没有                             |
    |          `queue.type`           | 用于事件缓冲的内部排队模型。指定`memory`传统基于内存的排队或`persisted`基于磁盘的确认排队（[持久队列](https://www.elastic.co/guide/en/logstash/current/persistent-queues.html)） |                           `memory`                           |
    |          `path.queue`           | 启用持续队列时数据文件存储的目录路径（`queue.type: persisted`）。 |                      ` path.data/queue`                      |
    |      `queue.page_capacity`      | 启用持久队列时使用的页面数据文件的大小（`queue.type: persisted`）。队列数据由分离为页面的仅附加数据文件组成。 |                             64mb                             |
    |        `queue.max_event`        | 启用持续队列时，队列中未读事件的最大数量（`queue.type: persisted`）。 |                          0（无限）                           |
    |        `queue.max_bytes`        | 队列的总容量（以字节数为单位）。确保磁盘驱动器的容量大于您在此处指定的值。如果同时`queue.max_events`和`queue.max_bytes`指定，Logstash采用的是先达到标准 |                         1024mb（1g）                         |
    |     `queue.checkpoint.acks`     | 启用持续队列时强制检查点之前的最大确认事件数（`queue.type: persisted`）。指定`queue.checkpoint.acks: 0`将此值设置为无限制 |                             1024                             |
    |    `queue.checkpoint.writes`    | 启用持久队列时强制检查点之前写入事件的最大数量（`queue.type: persisted`）。指定`queue.checkpoint.writes: 0`将此值设置为无限制 |                             1024                             |
    |          `queue_drain`          |       启用后，Logstash会在关闭之前等待持续队列被排空。       |                                                              |
    |   `dead_letter_queue.enable`    |           指示Logstash启用插件支持的DLQ功能的标志            |                            false                             |
    | ` dead_letter_queue.max_bytes ` | 每个死信队列的最大大小。如果这些条目超出此设置将增加死信队列的大小，则条目将被丢弃 |                            1024mb                            |
    |    `path.dead_letter_queue`     |             数据文件将存储在死信队列中的目录路径             |                `path.data/dead_letter_queue`                 |
    |           `http.host`           |                   指标REST端点的绑定地址。                   |                       ` "127.0.0.1" `                        |
    |           `http.port`           |                    指标REST端点的绑定端口                    |                             9600                             |
    |          ` log.level`           | 日志级别。有效的选项是：`fatal`  `error`  `warn`  `info`  `debug`  `trace` |                             info                             |
    |          `log.format`           | 日志格式。设置为`json`登录JSON格式，或`plain`使用`Object#.inspect` |                            plain                             |
    |           `path.logs`           |                 Logstash将其日志写入的目录。                 |                      LOGSTASH_HOME/logs                      |
    |         `path.plugins`          | 在哪里可以找到自定义插件。您可以多次指定此设置以包含多个路径。插件预计将在一个特定的目录层次结构：`PATH/logstash/TYPE/NAME.rb`其中`TYPE`是`inputs`，`filters`，`outputs`，或`codecs`，并且`NAME`是插件的名称 | 特定于平台的。请参阅[Logstash目录布局](https://www.elastic.co/guide/en/logstash/current/dir-layout.html)。 |

*   pipelines.yml

    包含在单个 Logstash 实例中运行多个管线的框架和说明

*   jvm.options

    包含 JVM 配置标志。使用此文件可以设置总堆空间的初始值和最大值。还可以使用此文件设置 Logstash 的区域设置。在单独的行上指定每个标志。

*   log4j2.properties

    包含库的默认设置。

*   startup.options

    linux 启动选项

###### 目录结构

* ElasticSearch 软件位置

    |  type   |                         description                          |          default location          |     setting      |
    | :-----: | :----------------------------------------------------------: | :--------------------------------: | :--------------: |
    |  home   |          elasticsearch home diretory or `$ES_HOME`           |     `/usr/share/elasticsearch`     |                  |
    |   bin   | 执行文件包含 elasticsearch 启动和 elasticsearch-plugin 安装插件 |   `/usr/share/elasticsearch/bin`   |                  |
    |  conf   |                    elasticsearch 配置文件                    |        `/etc/elasticsearch`        | `ES_PATCH_CONGF` |
    |  conf   | Environment variables including heap size, file descriptors. |    `/etc/default/elasticsearch`    |                  |
    |  data   | The location of the data files of each index / shard allocated on the node. Can hold multiple locations |      `/var/lib/elasticsearch`      |   `path.data`    |
    |  logs   |                      Log files location                      |      `/var/log/elasticsearch`      |    `path.log`    |
    | plugins | Plugin files location. Each plugin will be contained in a subdirectory | `/usr/share/elasticsearch/plugins` |                  |
    |  repo   | Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. |           Not configured           |   `path.repo`    |

* kibana 目录

    |     type     |                         description                          |       default location       |   setting   |
    | :----------: | :----------------------------------------------------------: | :--------------------------: | :---------: |
    |     home     |           Kibana home directory or `$KIBANA_HOME`            |     `/usr/share/kibana`      |             |
    |     bin      | Binary scripts including `kibana` to start the Kibana server and `kibana-plugin` to install plugins |   ` /usr/share/kibana/bin`   |             |
    |    config    |          Configuration files including `kibana.yml`          |        `/etc/kibana`         |             |
    |     data     | The location of the data files written to disk by Kibana and its plugins |      `/var/lib/kibana`       | `path.data` |
    | **optimize** | Transpiled source code. Certain administrative actions (e.g. plugin install) result in the source code being retranspiled on the fly. | `/usr/share/kibana/optimize` |             |
    | **plugins**  | Plugin files location. Each plugin will be contained in a subdirectory | `/usr/share/kibana/plugins`  |             |

*   Logstash 目录结构

    |   类型   |                     描述                      |           默认位置            |             设置              |
    | :------: | :-------------------------------------------: | :---------------------------: | :---------------------------: |
    |   home   |                  软件家目录                   |     `/usr/share/logstash`     |                               |
    |   bin    |                二进制脚本目录                 |   `/usr/share/logstash/bin`   |                               |
    | settings | 配置文件（包含 logstash.yml, 与 jvm.options)  |        `/etc/logstash`        |        `path.settings`        |
    |   conf   |             logstash 管道配置文件             | `/etc/logstash/conf.d/*.conf` | `/etc/logstash/pipelines.yml` |
    |   logs   |                 日志文件目录                  |      `/var/log/logstash`      |          `path.logs`          |
    | plugins  |   本地，非Ruby-Gem 模块文件，仅用于开发环境   | `/usr/share/logstash/plugins` |        `path.plugins`         |
    |   data   | logstash 及其插件用于任何持久性需求的数据文件 |      `/var/lib/logstash`      |         ` path.data`          |

    ```shell
    # 测试配置
    bin/logstash -f first-pipeline.conf --config.test_and_exit
    # 启动并指定配置，--config.reload.automatic 允许自动重载配置
    bin/logstash -f first-pipeline.conf --config.reload.automatic
    ```

##### 配置

###### es 相关配置

*   集群环境下 *elasticsearch.yml* 文件选项配置

    *   ElasticStack 6.4 以前

        ```yaml
        # 每个节点的集群名称需要一致
        cluster.name: work
        # 节点名称，每个节点不一样
        node.name: node-1
        # 可访问网络地址，每个节点物理地址
        network.host: 192.168.10.10
        # http 请求端口默认 9200
        http.port: 9200
        # 节点发现网络地址,节点通信端口默认为 9300
        discovery.zen.ping.unicast.hosts: ["host1:9300", "host2:9300"]
        # 可被选举为主节点的数量,为了避免脑裂个数为 master-eligible nodes / 2 + 1 
        discovery.zen.minimum_master_nodes: 2
        # x-pack 配置
        xpack.security.enabled: false
        ```

    *   ElasticStack 6.4 以后

        ```yaml
        cluster.name: <cluster.name>
        node.name: <node.name>
        path.data: /var/lib/elasticsearch
        path.logs: /var/log/elasticsearch
        network.host: 0.0.0.0
        http.port: 9200
        http.cors.enabled: true
        http.cors.allow-origin: "*"
        # 发现节点
        discovery.seed_hosts:
          - <node.ip:port>
          - <node.ip:port>
          - <node.ip:port>
        # 可以被选举为主节点的主机
        cluster.initial_master_nodes:
          - <node.name>
          - <node.name>
          - <node.name>
        ```

        必须是新启动节点，如果已生成节点后再配置 `discovery.seed_hosts` 则无法发现节点，因为原

        来的集群 id 不同。删除 `/var/lib/elasticsearch` 中文件即可
        
    *   集群 compose.yml

        ```yaml
        version: '2.2'
        services:
          es01:
            image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
            container_name: es01
            environment:
              - node.name=es01
              - cluster.name=es-docker-cluster
              - discovery.seed_hosts=es02,es03
              - cluster.initial_master_nodes=es01,es02,es03
              - bootstrap.memory_lock=true
              - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
            ulimits:
              memlock:
                soft: -1
                hard: -1
            volumes:
              - data01:/usr/share/elasticsearch/data
            ports:
              - 9200:9200
            networks:
              - elastic
        
          es02:
            image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
            container_name: es02
            environment:
              - node.name=es02
              - cluster.name=es-docker-cluster
              - discovery.seed_hosts=es01,es03
              - cluster.initial_master_nodes=es01,es02,es03
              - bootstrap.memory_lock=true
              - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
            ulimits:
              memlock:
                soft: -1
                hard: -1
            volumes:
              - data02:/usr/share/elasticsearch/data
            ports:
              - 9201:9201
            networks:
              - elastic
        
          es03:
            image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
            container_name: es03
            environment:
              - node.name=es03
              - cluster.name=es-docker-cluster
              - discovery.seed_hosts=es01,es02
              - cluster.initial_master_nodes=es01,es02,es03
              - bootstrap.memory_lock=true
              - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
            ulimits:
              memlock:
                soft: -1
                hard: -1
            volumes:
              - data03:/usr/share/elasticsearch/data
            ports:
              - 9202:9202
            networks:
              - elastic
        
          kib01:
            image: docker.elastic.co/kibana/kibana:7.9.2
            container_name: kib01
            ports:
              - 5601:5601
            environment:
              ELASTICSEARCH_URL: http://es01:9200
              ELASTICSEARCH_HOSTS: http://es01:9200
            networks:
              - elastic
        
        volumes:
          data01:
            driver: local
          data02:
            driver: local
          data03:
            driver: local
        
        networks:
          elastic:
            driver: bridge
        ```

*   安全相关配置

    1.  仅将 host 绑定到本地或内网地址

        ```yaml
        # elasticsearch.yml
        network.host: 127.0.0.1  // 或内网地址
        ```

    2.  防火墙相关配置

        ```shell
        # 限制9200-集群对外访问端口
        iptables -A INPUT -i eth0 -p tcp --destination-port 9200 -s {PUBLIC-IP-ADDRESS-HERE} -j DROP
        # 限制9200-集群对外访问端口
        iptables -A INPUT -i eth0 -p tcp --destination-port 9300 -s {PUBLIC-IP-ADDRESS-HERE} -j DROP
        # 限制5601-kibana访问端口
        iptables -A INPUT -i eth0 -p tcp --destination-port 5601 -s {PUBLIC-IP-ADDRESS-HERE} -j DROP
        ```

    3.  两天机器建立安全 SSH 隧道

        ```shell
        ssh -Nf -L 9200:localhost:9200 user@remote-elasticsearch-server
        curl http://localhost:9200/_search
        ```

    4.  nginx 代理配置

        ```shell
        # 生成密码文件
        printf "esuser:$(openssl passwd -crypt MySecret)\n" > /etc/nginx/passwords
        # 生成自签名SSL证书
        sudo mkdir /etc/nginx/ssl
        sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt
        ```

        nginx 代理配置

        ```nginx
        http {
            upstream elasticsearch {
                server 127.0.0.1:9200;
            }
        }
        server {
            # enable TLS
            listen 0.0.0.0:443 ssl;
            ssl_certificate /etc/nginx/ssl/nginx.crt;
            ssl_certificate_key /etc/nginx/ssl/nginx.key;
            ssl_protocols TLSv1.2;
            ssl_prefer_server_ciphers on;
            ssl_session_timeout 5m;
            ssl_ciphers "HIGH:!aNULL:!MD5 or HIGH:!aNULL:!MD5:!3DES";
            # Proxy for Elasticsearch
            location / {
                auth_basic "Login";
                auth_basic_user_file passwords;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header Host $http_host;
                proxy_set_header X-NginX-Proxy true;
                # use defined upstream with the name "elasticsearch"
                proxy_pass http://elasticsearch/;
                proxy_redirect off;
                if ($request_method = OPTIONS ) {
                    add_header Access-Control-Allow-Origin "*"; 
                    add_header Access-Control-Allow-Methods "GET, POST, , PUT, OPTIONS";
                    add_header Access-Control-Allow-Headers "Content-Type,Accept,Authorization, x-requested-with"; 
                    add_header Access-Control-Allow-Credentials "true"; 
                    add_header Content-Length 0;
                    add_header Content-Type application/json;
                    return 200;
            }
        }
        ```

#### Elasticsearch

##### 分布式特性

屏蔽了分布式系统的复杂性

-   分配文档到不同的容器 或 *分片* 中，文档可以储存在一个或多个节点中
-   按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡
-   复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失
-   将集群中任一节点的请求路由到存有相关数据的节点
-   集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复

一个运行中的 Elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 `cluster.name` 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据

当一个节点被选举成为 *主* 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。只有一个节点时它也是主节点

```
# 集群健康
_cluster/health
```

`status` 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：

-   **`green`**

    所有的主分片和副本分片都正常运行。

-   **`yellow`**

    所有的主分片都正常运行，但不是所有的副本分片都正常运行。

-   **`red`**

    有主分片没能正常运行。

###### 分片

一个 *分片* 是一个底层的 *工作单元* ，它仅保存了全部数据中的一部分。一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 文档被存储和索引到分片内，应用程序是直接与索引而不是与分片进行交互

Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里

一个分片可以是：

*    *主* 分片

    索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量（技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长）

*   副本分片

    是主分片的拷贝，硬件故障的冗余备份，并提供搜索和返回文档等读操作服务，节点丢失后，其上主分片在其他节点的副分片会提升为主分片

在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改（在相同节点数目的集群上增加更多的副本分片不能提高性能）

```
# 修改副分配
PUT /{index}/_settings
{
	"number_of_replicas": {num}
}
```

##### 数据

###### 文档

元数据

*   _index

    索引是逻辑上的命名空间，由一个或多个分片组合在一起。必须小写，不能以下划线开头，不能包含逗号

*   _id

    字符串，在索引中标识文档，可以自己提供 _id 或自动生成 id（生成 ID 是 URL-safe，基于 Base64 编码且长度为 20 个字符的 GUID 字符串）

索引文档使用 POST 方法，支持以下方式索引文档：`/{index}/_doc/{id}`、`/{index}/_doc`、`/{index}/_create/{id}`





